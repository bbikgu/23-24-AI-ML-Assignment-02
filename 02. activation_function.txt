2강 활성화 함수


퍼셉트론의 문제 : 단층으로 쌓으면 XOR게이트도 구현 못함


머신러닝의 학습 : 미분을 통해 학습
기존의 Heaviside함수 -> 미분 불가능. 된다 해도 다 0
-> 러닝에 도움이 안됨


미분이 가능하게 하는 법 -> 그래프를 부드럽게 만들어 주면 됨

활성화 함수(다음 뉴런으로의 전이를 활성화) -> 다음 뉴런으로의 전이를 할때 0과 1만 취하는게 아니라 그 사이에 있는 값들도 취할 수 있게 됨.


Sigmoid함수 f(x) = 


 = 1,  = 0


y` = - x ) ->  
y(0) = ½


 - 1  if x  0 /  - 1  if x  0


벡터에 대해서도 적용 가능.


딥러닝에선 sigmoid함수를 쓰면 문제가 생김
-> ReLU 함수로 해결


ReLU : 음수일때는 0, 양수일때는 y=x그래프